# Enhanced State Representation 在 GMZZ 地图上的提升分析

## GMZZ 地图特点

**地图名称**: 关门捉贼 (GMZZ - Guan Men Zhuo Zei)

**战术核心**: 通过形成包围圈，切断敌人退路，集中优势兵力歼灭敌人

**关键机制**:
1. **Supply Depot 控制**: 单位可以控制 Supply Depot 的升降（DepotLower/DepotRaise），用于阻挡或放行
2. **位置协调**: 需要多个单位协调形成包围圈
3. **时机把握**: 需要在合适的时机关闭"门"（Supply Depot），切断敌人退路
4. **空间感知**: 需要准确感知单位位置、敌人位置、Supply Depot 状态

## 性能对比

### 关键指标

| 指标 | dTAPE | Enhanced State Representation | 提升 |
|------|-------|------------------------------|------|
| **最高胜率** | 43.75% | 68.75% | +57.1% |
| **平均胜率** | 27.32% | 31.56% | +15.5% |
| **超过50%的次数** | 4/198 (2.0%) | 45/198 (22.7%) | +10.4倍 |
| **超过60%的次数** | 1/198 (0.5%) | 17/198 (8.6%) | +16倍 |
| **超过40%的稳定性** | 27.8% | 41.4% | +48.9% |

### 训练稳定性分析

- **dTAPE**: 胜率波动大，主要集中在20%-45%之间，难以稳定达到高胜率
- **Enhanced**: 胜率更稳定，能够更频繁地达到50%以上，最高达到68.75%

## 为什么 Enhanced State Representation 在 GMZZ 上特别有效？

### 1. **复杂空间状态感知需求**

**GMZZ 的核心挑战**:
- 需要同时感知多个单位的位置（己方单位、敌人、Supply Depot）
- 需要理解空间关系（包围圈是否形成、敌人是否在包围圈内）
- 需要判断 Supply Depot 的状态（升起/降下）及其战术意义

**dTAPE 的局限性**:
- 单层全连接网络难以提取复杂的空间特征
- 无法有效区分关键状态信息（如 Supply Depot 状态 vs 单位位置）
- 对多单位协调关系的理解能力有限

**Enhanced State Representation 的优势**:
- **多层特征提取**: 能够提取更抽象的空间关系特征
  - 第一层：提取基础位置、状态特征
  - 第二层：提取单位间关系、包围圈形成等高级特征
- **注意力机制**: 自动关注关键状态信息
  - 在需要关闭"门"时，关注 Supply Depot 状态
  - 在形成包围圈时，关注单位位置和敌人位置
  - 在攻击阶段，关注敌人血量和己方单位状态

### 2. **多单位协调决策需求**

**GMZZ 的战术要求**:
- 多个单位需要协调行动，形成包围圈
- 需要判断何时关闭 Supply Depot（"关门"）
- 需要判断何时集中攻击（"捉贼"）

**dTAPE 的问题**:
- 简单的状态编码难以捕捉单位间的协调关系
- 无法有效区分"形成包围圈"和"分散行动"的状态差异
- 对战术时机的判断能力弱

**Enhanced State Representation 的改进**:
- **特征注意力**: 能够动态关注不同单位的状态
  - 在包围阶段，关注外围单位的位置
  - 在攻击阶段，关注攻击单位的血量和位置
  - 在控制阶段，关注 Supply Depot 的状态
- **深层特征表示**: 能够学习到"包围圈形成"、"敌人被困"等高级战术状态
  - 通过多层网络提取单位位置的空间分布特征
  - 学习到 Supply Depot 状态与战术阶段的关系

### 3. **时序决策的稳定性**

**GMZZ 的决策特点**:
- 需要根据当前状态判断战术阶段（包围准备 → 形成包围 → 关闭门 → 集中攻击）
- 不同阶段的决策重点不同
- 需要稳定的状态表示来支持时序决策

**dTAPE 的训练不稳定性**:
- 胜率波动大（标准差 0.1574）
- 难以稳定达到高胜率（只有 2.0% 的时间超过 50%）
- 说明状态表示不够稳定，导致决策不稳定

**Enhanced State Representation 的稳定性提升**:
- **LayerNorm**: 稳定了特征分布，减少了训练过程中的状态表示变化
- **Dropout**: 防止过拟合，提高了泛化能力
- **多层特征提取**: 提供了更稳定的特征表示，减少了噪声影响
- 结果：胜率更稳定（超过 50% 的时间从 2.0% 提升到 22.7%）

### 4. **Supply Depot 机制的特殊性**

**Supply Depot 的战术意义**:
- 升降状态直接影响战术执行
- 需要准确感知 Depot 状态并判断何时操作
- 状态变化（升/降）是关键的战术信号

**为什么需要增强状态表示**:
- Supply Depot 状态是离散的（升起/降下），但战术意义是连续的（阻挡程度、时机重要性）
- 需要将离散状态编码为有意义的特征表示
- 注意力机制能够关注 Depot 状态变化，判断战术时机

### 5. **状态空间的复杂性**

**GMZZ 的状态空间特点**:
- **单位类型多样**: 己方单位、敌人、Supply Depot（两种状态）
- **位置信息关键**: 单位位置、敌人位置、Depot 位置
- **状态组合复杂**: 不同单位类型 × 不同位置 × Depot 状态 = 大量状态组合

**dTAPE 的表达能力限制**:
- 单层网络难以学习复杂的状态组合
- 无法有效区分相似但战术意义不同的状态

**Enhanced State Representation 的表达能力提升**:
- **多层特征提取**: 能够学习到复杂的状态组合
  - 第一层：学习基础特征（位置、类型、状态）
  - 第二层：学习组合特征（包围圈、战术阶段）
- **注意力机制**: 能够动态关注不同状态组合的重要性
  - 在包围阶段，关注位置组合
  - 在攻击阶段，关注状态组合

## 具体提升机制

### 1. **状态编码能力提升**

```
原始观测 [单位位置, 敌人位置, Depot状态, 单位血量, ...]
    ↓
Enhanced State Encoder
    ├─ 多层特征提取 → 提取空间关系特征
    └─ 注意力机制 → 关注关键状态（Depot状态、单位位置）
    ↓
增强特征 [包围圈特征, 战术阶段特征, 协调关系特征, ...]
    ↓
RNN → 时序决策
    ↓
Q值 → 动作选择（移动、攻击、控制Depot）
```

### 2. **注意力机制的动态关注**

- **包围准备阶段**: 注意力关注外围单位位置，判断包围圈是否形成
- **关门阶段**: 注意力关注 Depot 状态和敌人位置，判断关门时机
- **攻击阶段**: 注意力关注敌人血量和己方单位状态，判断攻击优先级

### 3. **训练稳定性的提升**

- **LayerNorm**: 稳定特征分布，减少训练波动
- **多层特征**: 提供更稳定的特征表示
- **结果**: 胜率稳定性从 27.8% 提升到 41.4%（超过 40% 的时间比例）

## 总结

Enhanced State Representation 在 GMZZ 地图上的显著提升主要源于：

1. **复杂空间感知**: 多层特征提取 + 注意力机制，能够更好地理解包围圈形成、Supply Depot 控制等复杂空间状态
2. **多单位协调**: 增强的状态表示能够捕捉单位间的协调关系，支持"关门捉贼"战术的执行
3. **训练稳定性**: LayerNorm 和 Dropout 提高了训练稳定性，使智能体能够更频繁地达到高胜率
4. **机制感知**: 注意力机制能够关注 Supply Depot 状态等关键机制，判断战术时机

这些改进使得 Enhanced State Representation 在需要复杂空间感知和多单位协调的 GMZZ 地图上表现显著优于 dTAPE 基线算法。

