{
  "artifacts": [],
  "command": "my_main",
  "experiment": {
    "base_dir": "/share/project/ytz/RLproject/StarCraft2_HLSMAC/RLalgs/HierarchicalArchitecture/src",
    "dependencies": [
      "numpy==1.26.4",
      "pyyaml==6.0.3",
      "sacred==0.8.7",
      "torch==2.1.2+cu118"
    ],
    "mainfile": "main.py",
    "name": "pymarl",
    "repositories": [
      {
        "commit": "ef65acb6db201cca6158afae4e1b05782e9b3ab0",
        "dirty": true,
        "url": "https://github.com/tandrewyang/stellar"
      },
      {
        "commit": "ef65acb6db201cca6158afae4e1b05782e9b3ab0",
        "dirty": true,
        "url": "https://github.com/tandrewyang/stellar"
      }
    ],
    "sources": [
      [
        "main.py",
        "_sources/main_3992b8ec14bf1a440a6c16ee0a45870f.py"
      ],
      [
        "utils/logging.py",
        "_sources/logging_b39327ec521d5576c7ca6fe2250525d5.py"
      ]
    ]
  },
  "fail_trace": [
    "Traceback (most recent call last):\n",
    "  File \"/root/miniconda3/envs/py310_sc2/lib/python3.10/site-packages/sacred/config/captured_function.py\", line 42, in captured_function\n    result = wrapped(*args, **kwargs)\n",
    "  File \"/share/project/ytz/RLproject/StarCraft2_HLSMAC/RLalgs/HierarchicalArchitecture/src/main.py\", line 53, in my_main\n    run_REGISTRY[_config['run']](_run, config, _log)\n",
    "  File \"/share/project/ytz/RLproject/StarCraft2_HLSMAC/RLalgs/HierarchicalArchitecture/src/run/run.py\", line 54, in run\n    run_sequential(args=args, logger=logger)\n",
    "  File \"/share/project/ytz/RLproject/StarCraft2_HLSMAC/RLalgs/HierarchicalArchitecture/src/run/run.py\", line 197, in run_sequential\n    learner.train(episode_sample, runner.t_env, episode)\n",
    "  File \"/share/project/ytz/RLproject/StarCraft2_HLSMAC/RLalgs/HierarchicalArchitecture/src/learners/max_q_learner.py\", line 123, in train\n    critic_agent_outs, (mu, sigma), logits, m_sample = self.critic_mac.forward(batch, t=t)\n",
    "  File \"/share/project/ytz/RLproject/StarCraft2_HLSMAC/RLalgs/HierarchicalArchitecture/src/controllers/cate_broadcast_comm_controller_full.py\", line 152, in forward\n    agent_outs, self.hidden_states = self.agent(agent_inputs, self.hidden_states)\n",
    "  File \"/root/miniconda3/envs/py310_sc2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n",
    "  File \"/root/miniconda3/envs/py310_sc2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n",
    "  File \"/share/project/ytz/RLproject/StarCraft2_HLSMAC/RLalgs/HierarchicalArchitecture/src/modules/agents/rnn_agent_n.py\", line 22, in forward\n    q = self.fc2(h)\n",
    "  File \"/root/miniconda3/envs/py310_sc2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n",
    "  File \"/root/miniconda3/envs/py310_sc2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n",
    "  File \"/root/miniconda3/envs/py310_sc2/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n    return F.linear(input, self.weight, self.bias)\n",
    "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 79.11 GiB of which 9.12 MiB is free. Process 2803249 has 8.21 GiB memory in use. Process 2804144 has 1.57 GiB memory in use. Process 2809808 has 31.62 GiB memory in use. Process 2811204 has 20.59 GiB memory in use. Process 3258839 has 6.62 GiB memory in use. Process 3269042 has 10.45 GiB memory in use. Of the allocated memory 9.24 GiB is allocated by PyTorch, and 373.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
  ],
  "heartbeat": "2025-11-24T03:07:33.578328",
  "host": {
    "ENV": {},
    "cpu": "Intel(R) Xeon(R) Platinum 8468",
    "gpus": {
      "driver_version": "535.161.08",
      "gpus": [
        {
          "model": "NVIDIA H100 80GB HBM3",
          "persistence_mode": true,
          "total_memory": 81559
        },
        {
          "model": "NVIDIA H100 80GB HBM3",
          "persistence_mode": true,
          "total_memory": 81559
        },
        {
          "model": "NVIDIA H100 80GB HBM3",
          "persistence_mode": true,
          "total_memory": 81559
        },
        {
          "model": "NVIDIA H100 80GB HBM3",
          "persistence_mode": true,
          "total_memory": 81559
        },
        {
          "model": "NVIDIA H100 80GB HBM3",
          "persistence_mode": true,
          "total_memory": 81559
        },
        {
          "model": "NVIDIA H100 80GB HBM3",
          "persistence_mode": true,
          "total_memory": 81559
        },
        {
          "model": "NVIDIA H100 80GB HBM3",
          "persistence_mode": true,
          "total_memory": 81559
        },
        {
          "model": "NVIDIA H100 80GB HBM3",
          "persistence_mode": true,
          "total_memory": 81559
        }
      ]
    },
    "hostname": "job-fcea379b-ab9f-4368-a257-c64368c4bfa2-master-0",
    "os": [
      "Linux",
      "Linux-5.15.0-105-generic-x86_64-with-glibc2.35"
    ],
    "python_version": "3.10.19"
  },
  "meta": {
    "command": "my_main",
    "config_updates": {
      "batch_size_run": 1,
      "env_args": {
        "map_name": "yqgz"
      },
      "save_model": true,
      "save_model_interval": 500000,
      "seed": 42,
      "t_max": 2005000,
      "use_tensorboard": true
    },
    "named_configs": [],
    "options": {
      "--beat-interval": null,
      "--capture": null,
      "--comment": null,
      "--debug": false,
      "--enforce_clean": false,
      "--file_storage": null,
      "--force": false,
      "--help": false,
      "--id": null,
      "--loglevel": null,
      "--mongo_db": null,
      "--name": null,
      "--pdb": false,
      "--print-config": false,
      "--priority": null,
      "--queue": false,
      "--s3": null,
      "--sql": null,
      "--tiny_db": null,
      "--unobserved": false,
      "COMMAND": null,
      "UPDATE": [
        "env_args.map_name=yqgz",
        "seed=42",
        "t_max=2005000",
        "batch_size_run=1",
        "use_tensorboard=True",
        "save_model=True",
        "save_model_interval=500000"
      ],
      "help": false,
      "with": true
    }
  },
  "resources": [],
  "result": null,
  "start_time": "2025-11-24T02:49:02.681771",
  "status": "FAILED",
  "stop_time": "2025-11-24T03:07:33.584340"
}