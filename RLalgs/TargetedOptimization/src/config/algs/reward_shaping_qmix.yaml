# Reward Shaping QMIX配置
# 奖励塑形：通过改进奖励信号提升学习效率

# use epsilon greedy action selector
action_selector: "epsilon_greedy"
epsilon_start: 0.995
epsilon_finish: 0.05
epsilon_anneal_time: 100000

runner: "episode"
batch_size_run: 1
buffer_size: 5000

batch_size: 128

critic_mac: "cate_broadcast_comm_mac_full"
critic_agent: "rnn_agent_n"

# Comm
comm: True
comm_embed_dim: 3
comm_method: "information_bottleneck_full"
c_beta: 1.
comm_beta: 0.001
comm_entropy_beta: 1e-6
gate_loss_beta: 0.00001
only_downstream: False
use_IB: True
is_print: False

is_comm_beta_decay: False
comm_beta_start_decay: 20000000
comm_beta_target: 1e-2
comm_beta_end_decay: 50000000

is_comm_entropy_beta_decay: False
comm_entropy_beta_start_decay: 20000000
comm_entropy_beta_target: 1e-4
comm_entropy_beta_end_decay: 50000000

is_cur_mu: False
is_rank_cut_mu: False
cut_mu_thres: 1.
cut_mu_rank_thres: 80.0

# update the target network every {} episodes
target_update_interval: 200
t_max: 2005000

# use the Q_Learner to train
mac : "basic_mac_logits"
agent: "rnn"
agent_output_type: "q"
learner: "reward_shaping_learner"  # 使用奖励塑形learner
double_q: True
mixer: "qmix"
mixing_embed_dim: 64
hypernet_layers: 2
hypernet_embed: 128

# 奖励塑形参数
reward_shaping_enabled: True  # 是否启用奖励塑形
reward_shaping_type: "potential_based"  # 塑形类型: "potential_based", "dense_reward", "curiosity"
reward_shaping_weight: 0.1  # 塑形奖励权重
reward_shaping_decay: 0.99  # 塑形权重衰减率（每步）

central_loss: 1
qmix_loss: 1
w: 0.5
hysteretic_qmix: True

central_mixing_embed_dim: 256
central_action_embed: 1
central_mac: "basic_central_mac"
central_agent: "central_rnn"
central_rnn_hidden_dim: 64
central_mixer: "ff"
td_lambda: 0.6
lr: 0.001

alpha_lr: 3e-4
alpha_init: -0.07

p: 0.5
name: "reward_shaping_qmix_env=4_adam_td_lambda"

