[INFO 23:53:18] pymarl Running command 'my_main'
[INFO 23:53:18] pymarl Started run with ID "1"
[DEBUG 23:53:18] pymarl Starting Heartbeat
[DEBUG 23:53:18] my_main Started
[INFO 23:53:18] my_main Experiment Parameters:
[INFO 23:53:18] my_main 

{   'action_selector': 'epsilon_greedy',
    'agent': 'rnn',
    'agent_output_type': 'q',
    'alpha_init': -0.07,
    'alpha_lr': '3e-4',
    'batch_size': 128,
    'batch_size_run': 1,
    'buffer_cpu_only': True,
    'buffer_size': 5000,
    'c_beta': 1.0,
    'central_action_embed': 1,
    'central_agent': 'central_rnn',
    'central_loss': 1,
    'central_mac': 'basic_central_mac',
    'central_mixer': 'ff',
    'central_mixing_embed_dim': 256,
    'central_rnn_hidden_dim': 64,
    'checkpoint_path': '',
    'comm': True,
    'comm_beta': 0.001,
    'comm_beta_end_decay': 50000000,
    'comm_beta_start_decay': 20000000,
    'comm_beta_target': '1e-2',
    'comm_embed_dim': 3,
    'comm_entropy_beta': '1e-6',
    'comm_entropy_beta_end_decay': 50000000,
    'comm_entropy_beta_start_decay': 20000000,
    'comm_entropy_beta_target': '1e-4',
    'comm_method': 'information_bottleneck_full',
    'critic_agent': 'rnn_agent_n',
    'critic_lr': 0.0005,
    'critic_mac': 'cate_broadcast_comm_mac_full',
    'cut_mu_rank_thres': 80.0,
    'cut_mu_thres': 1.0,
    'double_q': True,
    'env': 'sc2_tactics',
    'env_args': {   'continuing_episode': False,
                    'debug': False,
                    'difficulty': '7',
                    'game_version': None,
                    'heuristic_ai': False,
                    'heuristic_rest': False,
                    'map_name': 'jctq',
                    'move_amount': 2,
                    'obs_all_health': True,
                    'obs_instead_of_state': False,
                    'obs_last_action': False,
                    'obs_own_health': True,
                    'obs_pathing_grid': True,
                    'obs_terrain_height': True,
                    'obs_timestep_number': False,
                    'replay_dir': '',
                    'replay_prefix': '',
                    'reward_death_value': 10,
                    'reward_defeat': 0,
                    'reward_negative_scale': 0.5,
                    'reward_only_positive': False,
                    'reward_scale': True,
                    'reward_scale_rate': 20,
                    'reward_sparse': False,
                    'reward_win': 200,
                    'seed': 42,
                    'state_last_action': True,
                    'state_timestep_number': False,
                    'step_mul': 8},
    'epsilon_anneal_time': 100000,
    'epsilon_finish': 0.05,
    'epsilon_start': 0.995,
    'evaluate': False,
    'gamma': 0.99,
    'gate_loss_beta': 1e-05,
    'grad_norm_clip': 10,
    'hypernet_embed': 64,
    'hypernet_layers': 2,
    'hysteretic_qmix': True,
    'is_comm_beta_decay': False,
    'is_comm_entropy_beta_decay': False,
    'is_cur_mu': False,
    'is_print': False,
    'is_rank_cut_mu': False,
    'label': 'default_label',
    'learner': 'max_q_learner',
    'learner_log_interval': 10000,
    'load_step': 0,
    'local_results_path': 'results',
    'log_interval': 10000,
    'lr': 0.001,
    'mac': 'basic_mac_logits',
    'mixer': 'qmix',
    'mixing_embed_dim': 32,
    'name': 'ow_qmix_env=4_adam_td_lambda',
    'obs_agent_id': True,
    'obs_last_action': True,
    'only_downstream': False,
    'optim_alpha': 0.99,
    'optim_eps': 1e-05,
    'p': 0.5,
    'qmix_loss': 1,
    'repeat_id': 1,
    'rnn_hidden_dim': 64,
    'run': 'default',
    'runner': 'episode',
    'runner_log_interval': 10000,
    'save_model': True,
    'save_model_interval': 500000,
    'save_replay': False,
    'seed': 42,
    't_max': 2005000,
    'target_update_interval': 200,
    'td_lambda': 0.6,
    'test_greedy': True,
    'test_interval': 10000,
    'test_nepisode': 32,
    'use_IB': True,
    'use_cuda': True,
    'use_tensorboard': True,
    'w': 0.5}

2025-11-18 23:53:19.382450: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-18 23:53:19.512029: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
----------------------
You create a JCTQ env!
----------------------
162
Mixer Size: 
253.379K
namespace(runner='episode', mac='basic_mac_logits', env='sc2_tactics', env_args={'continuing_episode': False, 'difficulty': '7', 'game_version': None, 'map_name': 'jctq', 'move_amount': 2, 'obs_all_health': True, 'obs_instead_of_state': False, 'obs_last_action': False, 'obs_own_health': True, 'obs_pathing_grid': True, 'obs_terrain_height': True, 'obs_timestep_number': False, 'reward_death_value': 10, 'reward_defeat': 0, 'reward_negative_scale': 0.5, 'reward_only_positive': False, 'reward_scale': True, 'reward_scale_rate': 20, 'reward_sparse': False, 'reward_win': 200, 'replay_dir': '', 'replay_prefix': '', 'state_last_action': True, 'state_timestep_number': False, 'step_mul': 8, 'seed': 42, 'heuristic_ai': False, 'heuristic_rest': False, 'debug': False}, batch_size_run=1, test_nepisode=32, test_interval=10000, test_greedy=True, log_interval=10000, runner_log_interval=10000, learner_log_interval=10000, t_max=2005000, use_cuda=True, buffer_cpu_only=True, use_tensorboard=True, save_model=True, save_model_interval=500000, checkpoint_path='', evaluate=False, load_step=0, save_replay=False, local_results_path='results', gamma=0.99, batch_size=128, buffer_size=5000, lr=0.001, critic_lr=0.0005, optim_alpha=0.99, optim_eps=1e-05, grad_norm_clip=10, agent='rnn', rnn_hidden_dim=64, obs_agent_id=True, obs_last_action=True, repeat_id=1, label='default_label', run='default', action_selector='epsilon_greedy', epsilon_start=0.995, epsilon_finish=0.05, epsilon_anneal_time=100000, critic_mac='cate_broadcast_comm_mac_full', critic_agent='rnn_agent_n', comm=True, comm_embed_dim=3, comm_method='information_bottleneck_full', c_beta=1.0, comm_beta=0.001, comm_entropy_beta='1e-6', gate_loss_beta=1e-05, only_downstream=False, use_IB=True, is_print=False, is_comm_beta_decay=False, comm_beta_start_decay=20000000, comm_beta_target='1e-2', comm_beta_end_decay=50000000, is_comm_entropy_beta_decay=False, comm_entropy_beta_start_decay=20000000, comm_entropy_beta_target='1e-4', comm_entropy_beta_end_decay=50000000, is_cur_mu=False, is_rank_cut_mu=False, cut_mu_thres=1.0, cut_mu_rank_thres=80.0, target_update_interval=200, agent_output_type='q', learner='max_q_learner', double_q=True, mixer='qmix', mixing_embed_dim=32, hypernet_layers=2, hypernet_embed=64, central_loss=1, qmix_loss=1, w=0.5, hysteretic_qmix=True, central_mixing_embed_dim=256, central_action_embed=1, central_mac='basic_central_mac', central_agent='central_rnn', central_rnn_hidden_dim=64, central_mixer='ff', td_lambda=0.6, alpha_lr='3e-4', alpha_init=-0.07, p=0.5, name='ow_qmix_env=4_adam_td_lambda', seed=42, device='cuda', unique_token='ow_qmix_env=4_adam_td_lambda__2025-11-18_23-53-18', n_agents=4, n_actions=16, state_shape=155, accumulated_episodes=None)
[INFO 23:53:20] my_main Beginning training for 2005000 timesteps
[INFO 23:53:20] absl Launching SC2: /share/project/ytz/StarCraftII/Versions/Base75689/SC2_x64 -listen 127.0.0.1 -port 45465 -dataDir /share/project/ytz/StarCraftII/ -tempDir /tmp/sc-7ulg818s/
[INFO 23:53:20] absl Connecting to: ws://127.0.0.1:45465/sc2api, attempt: 0, running: True
Version: B75689 (SC2.4.10)
Build: Aug 12 2019 17:16:57
Command Line: '"/share/project/ytz/StarCraftII/Versions/Base75689/SC2_x64" -listen 127.0.0.1 -port 45465 -dataDir /share/project/ytz/StarCraftII/ -tempDir /tmp/sc-7ulg818s/'
Starting up...
Startup Phase 1 complete
[INFO 23:53:21] absl Connecting to: ws://127.0.0.1:45465/sc2api, attempt: 1, running: True
Startup Phase 2 complete
Creating stub renderer...
Listening on: 127.0.0.1:45465
Startup Phase 3 complete. Ready for commands.
[INFO 23:53:22] absl Connecting to: ws://127.0.0.1:45465/sc2api, attempt: 2, running: True
ConnectHandler: Request from 127.0.0.1:44066 accepted
ReadyHandler: 127.0.0.1:44066 ready
Requesting to join a single player game
Configuring interface options
Configure: raw interface enabled
Configure: feature layer interface disabled
Configure: score interface disabled
Configure: render interface disabled
Launching next game.
Next launch phase started: 2
Next launch phase started: 3
Next launch phase started: 4
Next launch phase started: 5
Next launch phase started: 6
Next launch phase started: 7
Next launch phase started: 8
Game has started.
Using default stable ids, none found at: /share/project/ytz/StarCraftII/stableid.json
Successfully loaded stable ids: GameData\stableid.json
Sending ResponseJoinGame
{'roach': 110, 'roachBurrowed': 118}
/share/project/ytz/RLproject/StarCraft2_HLSMAC/RLalgs/dTAPE/src/components/episode_buffer.py:93: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)
  target["filled"][slices] = 1
/share/project/ytz/RLproject/StarCraft2_HLSMAC/RLalgs/dTAPE/src/components/episode_buffer.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
  v = th.tensor(v, dtype=dtype, device=self.device)
/share/project/ytz/RLproject/StarCraft2_HLSMAC/RLalgs/dTAPE/src/components/episode_buffer.py:104: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)
  self._check_safe_view(v, target[k][_slices])
/share/project/ytz/RLproject/StarCraft2_HLSMAC/RLalgs/dTAPE/src/components/episode_buffer.py:105: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)
  target[k][_slices] = v.view_as(target[k][_slices])
/share/project/ytz/RLproject/StarCraft2_HLSMAC/RLalgs/dTAPE/src/components/episode_buffer.py:109: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)
  v = target[k][_slices]
/share/project/ytz/RLproject/StarCraft2_HLSMAC/RLalgs/dTAPE/src/components/episode_buffer.py:112: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)
  target[new_k][_slices] = v.view_as(target[new_k][_slices])
/share/project/ytz/RLproject/StarCraft2_HLSMAC/RLalgs/dTAPE/src/components/episode_buffer.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  v = th.tensor(v, dtype=dtype, device=self.device)
[INFO 23:53:27] my_main t_env: 22 / 2005000
[INFO 23:53:27] my_main Estimated time left: 20 minutes, 55 seconds. Time passed: 6 seconds
[INFO 23:53:32] my_main Saving models to results/models/ow_qmix_env=4_adam_td_lambda__2025-11-18_23-53-18/22
/share/project/ytz/RLproject/StarCraft2_HLSMAC/RLalgs/dTAPE/src/components/episode_buffer.py:151: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)
  new_data.transition_data[k] = v[item]
[INFO 23:54:31] my_main Updated target network
[INFO 23:56:03] my_main Updated target network
[INFO 23:56:30] my_main Recent Stats | t_env:      10015 | Episode:      460
alpha:                     0.9323	alpha_loss:               -4.9515	battle_won_mean:           0.0000	central_loss:              0.0672
comm_loss:                 0.1283	dead_allies_mean:          4.0000	dead_enemies_mean:         0.0000	delta_ally_mean:         290.0000
delta_deaths_mean:       -20.0000	delta_enemy_mean:         63.5000	ep_length_mean:           22.0000	epsilon:                   0.9950
grad_norm:                 1.1806	loss:                      0.2394	mixer_norm:                0.2804	policy_loss:              -0.3173
q_taken_mean:             -0.0319	qmix_loss:                 0.0438	reward_mean:              -3.3537	reward_std:                0.0000
target_mean:              -0.0433	td_error_abs:              0.2026	test_battle_won_mean:      0.0000	test_dead_allies_mean:     4.0000
test_dead_enemies_mean:    0.3750	test_delta_ally_mean:    290.0000	test_delta_deaths_mean:  -16.2500	test_delta_enemy_mean:    62.9922
test_ep_length_mean:      22.3438	test_reward_mean:         -3.3096	test_reward_std:           0.6666	w_to_use:                  0.7047

[INFO 23:56:31] my_main t_env: 10036 / 2005000
[INFO 23:56:31] my_main Estimated time left: 10 hours, 11 minutes, 19 seconds. Time passed: 3 minutes, 10 seconds
[INFO 23:57:53] my_main Updated target network
[INFO 23:59:58] my_main Updated target network
[INFO 00:01:06] my_main Recent Stats | t_env:      20015 | Episode:      929
alpha:                     0.8902	alpha_loss:               -4.9567	battle_won_mean:           0.0000	central_loss:              0.0034
comm_loss:                 0.0365	dead_allies_mean:          4.0000	dead_enemies_mean:         0.0065	delta_ally_mean:         290.0000
delta_deaths_mean:       -19.9348	delta_enemy_mean:        119.2636	ep_length_mean:           21.7696	epsilon:                   0.9004
grad_norm:                 0.1685	loss:                      0.0475	mixer_norm:                0.0534	policy_loss:              -0.8492
q_taken_mean:             -0.1315	qmix_loss:                 0.0076	reward_mean:              -2.5942	reward_std:                0.4418
target_mean:              -0.1373	td_error_abs:              0.0807	test_battle_won_mean:      0.0000	test_dead_allies_mean:     4.0000
test_dead_enemies_mean:    0.4688	test_delta_ally_mean:    290.0000	test_delta_deaths_mean:  -15.3125	test_delta_enemy_mean:   246.4102
test_ep_length_mean:      21.0625	test_reward_mean:         -0.8014	test_reward_std:           0.2643	w_to_use:                  0.7347

[INFO 00:01:07] my_main t_env: 20037 / 2005000
[INFO 00:01:07] my_main Estimated time left: 15 hours, 14 minutes, 2 seconds. Time passed: 7 minutes, 46 seconds
[INFO 00:02:02] my_main Updated target network
[INFO 00:04:10] my_main Updated target network
[INFO 00:06:30] my_main Updated target network
[INFO 00:06:32] my_main Recent Stats | t_env:      30032 | Episode:     1403
alpha:                     0.8492	alpha_loss:               -4.9602	battle_won_mean:           0.0000	central_loss:              0.0032
comm_loss:                 0.0275	dead_allies_mean:          4.0000	dead_enemies_mean:         0.0107	delta_ally_mean:         290.0000
delta_deaths_mean:       -19.8934	delta_enemy_mean:        141.8777	ep_length_mean:           21.3241	epsilon:                   0.8059
grad_norm:                 0.2320	loss:                      0.0393	mixer_norm:                0.1549	policy_loss:              -0.8686
q_taken_mean:             -0.1983	qmix_loss:                 0.0086	reward_mean:              -2.2859	reward_std:                0.4442
target_mean:              -0.2017	td_error_abs:              0.0854	test_battle_won_mean:      0.0000	test_dead_allies_mean:     4.0000
test_dead_enemies_mean:    0.5938	test_delta_ally_mean:    290.0000	test_delta_deaths_mean:  -14.0625	test_delta_enemy_mean:   241.8438
test_ep_length_mean:      21.0312	test_reward_mean:         -0.8465	test_reward_std:           0.2104	w_to_use:                  0.7623

[INFO 00:06:33] my_main t_env: 30053 / 2005000
[INFO 00:06:33] my_main Estimated time left: 17 hours, 50 minutes, 19 seconds. Time passed: 13 minutes, 12 seconds
[INFO 00:09:07] my_main Updated target network
[INFO 00:11:53] my_main Updated target network
[INFO 00:13:00] my_main Recent Stats | t_env:      40051 | Episode:     1882
alpha:                     0.8097	alpha_loss:               -4.9691	battle_won_mean:           0.0000	central_loss:              0.0024
comm_loss:                 0.0503	dead_allies_mean:          4.0000	dead_enemies_mean:         0.0274	delta_ally_mean:         290.0000
delta_deaths_mean:       -19.7257	delta_enemy_mean:        158.6429	ep_length_mean:           21.1308	epsilon:                   0.7112
grad_norm:                 0.3509	loss:                      0.0614	mixer_norm:                0.3189	policy_loss:              -0.7152
q_taken_mean:             -0.2114	qmix_loss:                 0.0087	reward_mean:              -2.0555	reward_std:                0.4303
target_mean:              -0.2100	td_error_abs:              0.0830	test_battle_won_mean:      0.0000	test_dead_allies_mean:     4.0000
test_dead_enemies_mean:    0.5312	test_delta_ally_mean:    290.0000	test_delta_deaths_mean:  -14.6875	test_delta_enemy_mean:   234.0938
test_ep_length_mean:      20.9688	test_reward_mean:         -0.9605	test_reward_std:           0.2183	w_to_use:                  0.8033

[INFO 00:13:01] my_main t_env: 40072 / 2005000
[INFO 00:13:01] my_main Estimated time left: 21 hours, 9 minutes, 28 seconds. Time passed: 19 minutes, 40 seconds
[INFO 00:14:25] my_main Updated target network
