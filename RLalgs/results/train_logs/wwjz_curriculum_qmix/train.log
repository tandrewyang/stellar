/root/miniconda3/envs/py310_sc2/lib/python3.10/site-packages/sacred/dependencies.py:11: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
[DEBUG 14:08:09] git.cmd Popen(['git', 'version'], cwd=/share/project/ytz/RLproject/StarCraft2_HLSMAC/RLalgs/CurriculumLearning, stdin=None, shell=False, universal_newlines=False)
[DEBUG 14:08:09] git.cmd Popen(['git', 'version'], cwd=/share/project/ytz/RLproject/StarCraft2_HLSMAC/RLalgs/CurriculumLearning, stdin=None, shell=False, universal_newlines=False)
[DEBUG 14:08:09] git.util sys.platform='linux', git_executable='git'
[DEBUG 14:08:09] git.cmd Popen(['git', 'diff', '--cached', '--abbrev=40', '--full-index', '--raw'], cwd=/share/project/ytz/RLproject/StarCraft2_HLSMAC, stdin=None, shell=False, universal_newlines=False)
[DEBUG 14:08:09] git.cmd Popen(['git', 'diff', '--abbrev=40', '--full-index', '--raw'], cwd=/share/project/ytz/RLproject/StarCraft2_HLSMAC, stdin=None, shell=False, universal_newlines=False)
[DEBUG 14:08:09] git.cmd Popen(['git', 'cat-file', '--batch-check'], cwd=/share/project/ytz/RLproject/StarCraft2_HLSMAC, stdin=<valid stream>, shell=False, universal_newlines=False)
[DEBUG 14:08:09] git.util sys.platform='linux', git_executable='git'
[DEBUG 14:08:09] git.cmd Popen(['git', 'diff', '--cached', '--abbrev=40', '--full-index', '--raw'], cwd=/share/project/ytz/RLproject/StarCraft2_HLSMAC, stdin=None, shell=False, universal_newlines=False)
[DEBUG 14:08:09] git.cmd Popen(['git', 'diff', '--abbrev=40', '--full-index', '--raw'], cwd=/share/project/ytz/RLproject/StarCraft2_HLSMAC, stdin=None, shell=False, universal_newlines=False)
[DEBUG 14:08:09] git.cmd Popen(['git', 'cat-file', '--batch-check'], cwd=/share/project/ytz/RLproject/StarCraft2_HLSMAC, stdin=<valid stream>, shell=False, universal_newlines=False)
[INFO 14:08:09] root Saving to FileStorageObserver in /share/project/ytz/RLproject/StarCraft2_HLSMAC/RLalgs/CurriculumLearning/results/sacred/wwjz/curriculum_qmix_env=4_adam_td_lambda.
[DEBUG 14:08:11] pymarl Using capture mode "fd"
[INFO 14:08:11] pymarl Running command 'my_main'
[INFO 14:08:11] pymarl Started run with ID "1"
[DEBUG 14:08:11] pymarl Starting Heartbeat
[DEBUG 14:08:11] my_main Started
[INFO 14:08:11] my_main Experiment Parameters:
[INFO 14:08:11] my_main 

{   'action_selector': 'epsilon_greedy',
    'agent': 'rnn',
    'agent_output_type': 'q',
    'alpha_init': -0.07,
    'alpha_lr': '3e-4',
    'batch_size': 128,
    'batch_size_run': 1,
    'buffer_cpu_only': True,
    'buffer_size': 5000,
    'c_beta': 1.0,
    'central_action_embed': 1,
    'central_agent': 'central_rnn',
    'central_loss': 1,
    'central_mac': 'basic_central_mac',
    'central_mixer': 'ff',
    'central_mixing_embed_dim': 256,
    'central_rnn_hidden_dim': 64,
    'checkpoint_path': '',
    'comm': True,
    'comm_beta': 0.001,
    'comm_beta_end_decay': 50000000,
    'comm_beta_start_decay': 20000000,
    'comm_beta_target': '1e-2',
    'comm_embed_dim': 3,
    'comm_entropy_beta': '1e-6',
    'comm_entropy_beta_end_decay': 50000000,
    'comm_entropy_beta_start_decay': 20000000,
    'comm_entropy_beta_target': '1e-4',
    'comm_method': 'information_bottleneck_full',
    'critic_agent': 'rnn_agent_n',
    'critic_lr': 0.0005,
    'critic_mac': 'cate_broadcast_comm_mac_full',
    'curriculum_enabled': True,
    'curriculum_end_step': 1000000,
    'curriculum_max_difficulty': 1.0,
    'curriculum_min_difficulty': 0.0,
    'curriculum_schedule': 'linear',
    'curriculum_start_step': 0,
    'cut_mu_rank_thres': 80.0,
    'cut_mu_thres': 1.0,
    'double_q': True,
    'env': 'sc2_tactics',
    'env_args': {   'continuing_episode': False,
                    'debug': False,
                    'difficulty': '7',
                    'game_version': None,
                    'heuristic_ai': False,
                    'heuristic_rest': False,
                    'map_name': 'wwjz',
                    'move_amount': 2,
                    'obs_all_health': True,
                    'obs_instead_of_state': False,
                    'obs_last_action': False,
                    'obs_own_health': True,
                    'obs_pathing_grid': True,
                    'obs_terrain_height': True,
                    'obs_timestep_number': False,
                    'replay_dir': '',
                    'replay_prefix': '',
                    'reward_death_value': 10,
                    'reward_defeat': 0,
                    'reward_negative_scale': 0.5,
                    'reward_only_positive': False,
                    'reward_scale': True,
                    'reward_scale_rate': 20,
                    'reward_sparse': False,
                    'reward_win': 200,
                    'seed': 42,
                    'state_last_action': True,
                    'state_timestep_number': False,
                    'step_mul': 8},
    'epsilon_anneal_time': 100000,
    'epsilon_finish': 0.05,
    'epsilon_start': 0.995,
    'evaluate': False,
    'gamma': 0.99,
    'gate_loss_beta': 1e-05,
    'grad_norm_clip': 10,
    'hypernet_embed': 128,
    'hypernet_layers': 2,
    'hysteretic_qmix': True,
    'is_comm_beta_decay': False,
    'is_comm_entropy_beta_decay': False,
    'is_cur_mu': False,
    'is_print': False,
    'is_rank_cut_mu': False,
    'label': 'default_label',
    'learner': 'curriculum_learner',
    'learner_log_interval': 10000,
    'load_step': 0,
    'local_results_path': 'results',
    'log_interval': 10000,
    'lr': 0.001,
    'mac': 'basic_mac_logits',
    'mixer': 'qmix',
    'mixing_embed_dim': 64,
    'name': 'curriculum_qmix_env=4_adam_td_lambda',
    'obs_agent_id': True,
    'obs_last_action': True,
    'only_downstream': False,
    'optim_alpha': 0.99,
    'optim_eps': 1e-05,
    'p': 0.5,
    'qmix_loss': 1,
    'repeat_id': 1,
    'rnn_hidden_dim': 64,
    'run': 'default',
    'runner': 'episode',
    'runner_log_interval': 10000,
    'save_model': True,
    'save_model_interval': 500000,
    'save_replay': False,
    'seed': 42,
    't_max': 2005000,
    'target_update_interval': 200,
    'td_lambda': 0.6,
    'test_greedy': True,
    'test_interval': 10000,
    'test_nepisode': 32,
    'use_IB': True,
    'use_cuda': True,
    'use_tensorboard': True,
    'w': 0.5}

----------------------
You create a WWJZ env!
----------------------
262
Mixer Size: 
497.923K
namespace(runner='episode', mac='basic_mac_logits', env='sc2_tactics', env_args={'continuing_episode': False, 'difficulty': '7', 'game_version': None, 'map_name': 'wwjz', 'move_amount': 2, 'obs_all_health': True, 'obs_instead_of_state': False, 'obs_last_action': False, 'obs_own_health': True, 'obs_pathing_grid': True, 'obs_terrain_height': True, 'obs_timestep_number': False, 'reward_death_value': 10, 'reward_defeat': 0, 'reward_negative_scale': 0.5, 'reward_only_positive': False, 'reward_scale': True, 'reward_scale_rate': 20, 'reward_sparse': False, 'reward_win': 200, 'replay_dir': '', 'replay_prefix': '', 'state_last_action': True, 'state_timestep_number': False, 'step_mul': 8, 'seed': 42, 'heuristic_ai': False, 'heuristic_rest': False, 'debug': False}, batch_size_run=1, test_nepisode=32, test_interval=10000, test_greedy=True, log_interval=10000, runner_log_interval=10000, learner_log_interval=10000, t_max=2005000, use_cuda=True, buffer_cpu_only=True, use_tensorboard=True, save_model=True, save_model_interval=500000, checkpoint_path='', evaluate=False, load_step=0, save_replay=False, local_results_path='results', gamma=0.99, batch_size=128, buffer_size=5000, lr=0.001, critic_lr=0.0005, optim_alpha=0.99, optim_eps=1e-05, grad_norm_clip=10, agent='rnn', rnn_hidden_dim=64, obs_agent_id=True, obs_last_action=True, repeat_id=1, label='default_label', run='default', action_selector='epsilon_greedy', epsilon_start=0.995, epsilon_finish=0.05, epsilon_anneal_time=100000, critic_mac='cate_broadcast_comm_mac_full', critic_agent='rnn_agent_n', comm=True, comm_embed_dim=3, comm_method='information_bottleneck_full', c_beta=1.0, comm_beta=0.001, comm_entropy_beta='1e-6', gate_loss_beta=1e-05, only_downstream=False, use_IB=True, is_print=False, is_comm_beta_decay=False, comm_beta_start_decay=20000000, comm_beta_target='1e-2', comm_beta_end_decay=50000000, is_comm_entropy_beta_decay=False, comm_entropy_beta_start_decay=20000000, comm_entropy_beta_target='1e-4', comm_entropy_beta_end_decay=50000000, is_cur_mu=False, is_rank_cut_mu=False, cut_mu_thres=1.0, cut_mu_rank_thres=80.0, target_update_interval=200, agent_output_type='q', learner='curriculum_learner', double_q=True, mixer='qmix', mixing_embed_dim=64, hypernet_layers=2, hypernet_embed=128, curriculum_enabled=True, curriculum_schedule='linear', curriculum_start_step=0, curriculum_end_step=1000000, curriculum_min_difficulty=0.0, curriculum_max_difficulty=1.0, central_loss=1, qmix_loss=1, w=0.5, hysteretic_qmix=True, central_mixing_embed_dim=256, central_action_embed=1, central_mac='basic_central_mac', central_agent='central_rnn', central_rnn_hidden_dim=64, central_mixer='ff', td_lambda=0.6, alpha_lr='3e-4', alpha_init=-0.07, p=0.5, name='curriculum_qmix_env=4_adam_td_lambda', seed=42, device='cuda', unique_token='curriculum_qmix_env=4_adam_td_lambda__2025-11-19_14-08-11', n_agents=8, n_actions=21, state_shape=322, accumulated_episodes=None)
[INFO 14:08:13] my_main Beginning training for 2005000 timesteps
[INFO 14:08:13] absl Launching SC2: /share/project/ytz/StarCraftII/Versions/Base75689/SC2_x64 -listen 127.0.0.1 -port 42697 -dataDir /share/project/ytz/StarCraftII/ -tempDir /tmp/sc-649m3vk0/
[INFO 14:08:13] absl Connecting to: ws://127.0.0.1:42697/sc2api, attempt: 0, running: True
Version: B75689 (SC2.4.10)
Build: Aug 12 2019 17:16:57
Command Line: '"/share/project/ytz/StarCraftII/Versions/Base75689/SC2_x64" -listen 127.0.0.1 -port 42697 -dataDir /share/project/ytz/StarCraftII/ -tempDir /tmp/sc-649m3vk0/'
Starting up...
Startup Phase 1 complete
[INFO 14:08:14] absl Connecting to: ws://127.0.0.1:42697/sc2api, attempt: 1, running: True
Startup Phase 2 complete
Creating stub renderer...
Listening on: 127.0.0.1:42697
Startup Phase 3 complete. Ready for commands.
[INFO 14:08:15] absl Connecting to: ws://127.0.0.1:42697/sc2api, attempt: 2, running: True
ConnectHandler: Request from 127.0.0.1:34528 accepted
ReadyHandler: 127.0.0.1:34528 ready
Requesting to join a single player game
Configuring interface options
Configure: raw interface enabled
Configure: feature layer interface disabled
Configure: score interface disabled
Configure: render interface disabled
Launching next game.
Next launch phase started: 2
Next launch phase started: 3
Next launch phase started: 4
Next launch phase started: 5
Next launch phase started: 6
Next launch phase started: 7
Next launch phase started: 8
Game has started.
Using default stable ids, none found at: /share/project/ytz/StarCraftII/stableid.json
Successfully loaded stable ids: GameData\stableid.json
Sending ResponseJoinGame
{'nexus': 59, 'zealot': 73}
/share/project/ytz/RLproject/StarCraft2_HLSMAC/RLalgs/CurriculumLearning/src/components/episode_buffer.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)
  v = th.tensor(v, dtype=dtype, device=self.device)
/share/project/ytz/RLproject/StarCraft2_HLSMAC/RLalgs/CurriculumLearning/src/components/episode_buffer.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  v = th.tensor(v, dtype=dtype, device=self.device)
[INFO 14:08:22] my_main t_env: 79 / 2005000
[INFO 14:08:22] my_main Estimated time left: 30 minutes, 18 seconds. Time passed: 9 seconds
[INFO 14:09:15] my_main Saving models to results/models/curriculum_qmix_env=4_adam_td_lambda__2025-11-19_14-08-11/79
[INFO 14:12:59] my_main Recent Stats | t_env:      10004 | Episode:      129
alpha:                     0.9323	alpha_loss:              -12.5269	battle_won_mean:           0.0000	central_loss:              0.0274
comm_loss:                 0.2654	curriculum/difficulty:     0.0100	curriculum/progress:       0.0100	dead_allies_mean:          1.0000
dead_enemies_mean:         0.0000	delta_ally_mean:         1000.0000	delta_deaths_mean:        -5.0000	delta_enemy_mean:          0.0000
ep_length_mean:           79.0000	epsilon:                   0.9950	grad_norm:                 1.7080	loss:                      0.3247
mixer_norm:                1.2774	policy_loss:              -0.2994	q_taken_mean:             -0.0124	qmix_loss:                 0.0319
reward_mean:              -7.0775	reward_std:                0.0000	target_mean:              -0.0323	td_error_abs:              0.1999
test_battle_won_mean:      0.0000	test_dead_allies_mean:     1.0000	test_dead_enemies_mean:    0.0000	test_delta_ally_mean:    1000.0000
test_delta_deaths_mean:   -5.0000	test_delta_enemy_mean:     0.0000	test_ep_length_mean:      77.4688	test_reward_mean:         -7.0775
test_reward_std:           0.0000	w_to_use:                  0.5931	
[INFO 14:13:01] my_main t_env: 10083 / 2005000
[INFO 14:13:01] my_main Estimated time left: 15 hours, 28 minutes, 57 seconds. Time passed: 4 minutes, 48 seconds
[INFO 14:17:40] my_main Updated target network
[INFO 14:20:23] my_main Recent Stats | t_env:      20021 | Episode:      258
alpha:                     0.9204	alpha_loss:              -12.5314	battle_won_mean:           0.0000	central_loss:              0.0030
comm_loss:                 0.0908	curriculum/difficulty:     0.0199	curriculum/progress:       0.0199	dead_allies_mean:          1.0000
dead_enemies_mean:         0.0000	delta_ally_mean:         1000.0000	delta_deaths_mean:        -5.0000	delta_enemy_mean:          0.0000
ep_length_mean:           77.5504	epsilon:                   0.9005	grad_norm:                 0.1226	loss:                      0.0972
mixer_norm:                0.0885	policy_loss:              -1.0630	q_taken_mean:             -0.0583	qmix_loss:                 0.0033
reward_mean:              -7.0775	reward_std:                0.0000	target_mean:              -0.0592	td_error_abs:              0.0458
test_battle_won_mean:      0.0000	test_dead_allies_mean:     1.0000	test_dead_enemies_mean:    0.0000	test_delta_ally_mean:    1000.0000
test_delta_deaths_mean:   -5.0000	test_delta_enemy_mean:     0.0000	test_ep_length_mean:      77.2812	test_reward_mean:         -7.0775
test_reward_std:           0.0000	w_to_use:                  0.6852	
[INFO 14:20:27] my_main t_env: 20098 / 2005000
[INFO 14:20:27] my_main Estimated time left: 1 days, 32 minutes, 34 seconds. Time passed: 12 minutes, 14 seconds
/root/miniconda3/envs/py310_sc2/lib/python3.10/site-packages/sacred/stdout_capturing.py:179: UserWarning: tee_stdout.wait timeout. Forcibly terminating.
  warnings.warn("tee_stdout.wait timeout. Forcibly terminating.")
/root/miniconda3/envs/py310_sc2/lib/python3.10/site-packages/sacred/stdout_capturing.py:185: UserWarning: tee_stderr.wait timeout. Forcibly terminating.
  warnings.warn("tee_stderr.wait timeout. Forcibly terminating.")
[DEBUG 14:24:50] pymarl Stopping Heartbeat
[ERROR 14:24:50] pymarl Failed after 0:16:39!
Traceback (most recent calls WITHOUT Sacred internals):
  File "/share/project/ytz/RLproject/StarCraft2_HLSMAC/RLalgs/CurriculumLearning/src/main.py", line 53, in my_main
    run_REGISTRY[_config['run']](_run, config, _log)
  File "/share/project/ytz/RLproject/StarCraft2_HLSMAC/RLalgs/CurriculumLearning/src/run/run.py", line 54, in run
    run_sequential(args=args, logger=logger)
  File "/share/project/ytz/RLproject/StarCraft2_HLSMAC/RLalgs/CurriculumLearning/src/run/run.py", line 197, in run_sequential
    learner.train(episode_sample, runner.t_env, episode)
  File "/share/project/ytz/RLproject/StarCraft2_HLSMAC/RLalgs/CurriculumLearning/src/learners/curriculum_learner.py", line 107, in train
    return super(CurriculumLearner, self).train(batch, t_env, episode_num)
  File "/share/project/ytz/RLproject/StarCraft2_HLSMAC/RLalgs/CurriculumLearning/src/learners/max_q_learner.py", line 271, in train
    policy_loss.backward(retain_graph=True)
  File "/root/miniconda3/envs/py310_sc2/lib/python3.10/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/root/miniconda3/envs/py310_sc2/lib/python3.10/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 44.00 MiB. GPU 0 has a total capacty of 79.11 GiB of which 33.31 MiB is free. Process 800898 has 11.87 GiB memory in use. Process 800714 has 17.58 GiB memory in use. Process 801172 has 11.87 GiB memory in use. Process 801510 has 930.00 MiB memory in use. Process 802152 has 1.58 GiB memory in use. Process 802640 has 4.52 GiB memory in use. Process 805453 has 19.07 GiB memory in use. Process 805882 has 5.27 GiB memory in use. Process 806347 has 946.00 MiB memory in use. Process 807019 has 4.51 GiB memory in use. Process 807486 has 936.00 MiB memory in use. Of the allocated memory 3.27 GiB is allocated by PyTorch, and 154.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

[INFO 14:24:50] absl Shutdown gracefully.
[INFO 14:24:50] absl Shutdown with return code: -15
